{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "#from random import randrange\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image helper ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "def get_image(i_dim, n_dim, n_pos , n_strength, pos_scale):\n",
    "    image = torch.zeros((3, i_dim,i_dim))\n",
    "    if isinstance(n_pos, torch.Tensor):\n",
    "        i_n_pos = (n_pos * pos_scale).type(torch.int)\n",
    "    else:\n",
    "        i_n_pos = (n_pos * pos_scale).astype(int)\n",
    "    #print(i_n_pos[0])\n",
    "    for c in range(3): #to get it equal over all chanels \n",
    "        image[c, i_n_pos[0].item():(i_n_pos[0].item() + n_dim),i_n_pos[1].item():(i_n_pos[1].item()+n_dim)] = n_strength\n",
    "    return image\n",
    "def net_show(n_pos, i_dim=64, n_dim=3, n_strength=torch.ones((3,3)), pos_scale=62):\n",
    "    with torch.no_grad():\n",
    "        npn_pos = n_pos.numpy()\n",
    "        npn_pos = np.transpose(npn_pos, (1,2,0))\n",
    "        for batch in npn_pos:\n",
    "            red_n_pos = np.array([batch[0][0], batch[1][0]])\n",
    "            print(red_n_pos)\n",
    "            image = get_image(i_dim, n_dim, red_n_pos, n_strength, pos_scale)\n",
    "            image_show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dim = 64\n",
    "n_dim = 3\n",
    "\n",
    "dataset_parameter = {\n",
    "    \"train\": (\"./pixel_finder_train_data.pth\", 2000), \n",
    "    \"test\": (\"./pixel_finder_test_data.pth\", 500),\n",
    "    \"validate\": (\"./pixel_finder_validate_data.pth\",500)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePositionDataset(Dataset):\n",
    "    def __init__(self, i_dim, n_dim, sample_size, path=\"\"):\n",
    "        self.i_dim = i_dim\n",
    "        self.n_dim = n_dim\n",
    "        self.pos_scale = i_dim-n_dim+1\n",
    "        if path != \"\": #load the data\n",
    "            self.load(path, i_dim, n_dim)\n",
    "            assert sample_size == self.sample_size\n",
    "        else: #create new data\n",
    "            self.sample_size = sample_size\n",
    "            self.data = []\n",
    "            for i in range(sample_size):\n",
    "                n_pos = torch.flatten(torch.rand((1,2)))\n",
    "                n_strength = torch.rand((3,3))#np.random.rand(n_dim, n_dim) #one chanel\n",
    "                self.data.append((n_pos, n_strength))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sample_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = (get_image(i_dim, n_dim, self.data[idx][0], self.data[idx][1], self.pos_scale), self.data[idx][0], self.pos_scale)\n",
    "        return sample\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.data, path)\n",
    "        \n",
    "    def load(self, path, i_dim, n_dim):\n",
    "        self.data = torch.load(path)\n",
    "        self.sample_size = len(self.data)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset\n",
    "dataset = {}\n",
    "dataloader = {}\n",
    "for name, parameter in dataset_parameter.items():\n",
    "    dataset[name] = ImagePositionDataset(i_dim, n_dim, parameter[1])\n",
    "    dataset[name].save(parameter[0])\n",
    "    dataloader[name] = DataLoader(dataset[name], batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataloader = {}\n",
    "for name, parameter in dataset_parameter.items():\n",
    "    dataset[name] = ImagePositionDataset(i_dim, n_dim, parameter[1], path=parameter[0])\n",
    "    dataloader[name] = DataLoader(dataset[name], batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(dataloader[\"train\"])\n",
    "images, labels, scales = dataiter.next()\n",
    "image_show(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,_,_ = dataset[\"train\"][0]\n",
    "image_show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test net on selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(images) \n",
    "#net(image.unsqueeze(0));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path = {}\n",
    "net = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetPath(net_choice, epoch, learnrate, original=False):\n",
    "    if original:\n",
    "        return f\"{net_path[net_choice]}.pth\"\n",
    "    return f\"{net_path[net_choice]}_lr_{learnrate}_ep_{epoch}.pth\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path[\"conv\"] = './pixel_finder_conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "\n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
    "\n",
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "    \n",
    "    \n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "\n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "    \n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv = ResNetLayer(3, 16, block=ResNetBasicBlock, n=2)\n",
    "        self.fc1 = nn.Linear(32*32*16 , 1024)\n",
    "        self.fc2 = nn.Linear(1024,2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.drop_out = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sig(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "net[\"conv\"] = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_path[\"linear\"] = './pixel_finder_linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(i_dim*i_dim*3, i_dim*i_dim*3)\n",
    "        self.fc2 = nn.Linear(i_dim*i_dim*3,2)\n",
    "        #self.drop_out = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net[\"linear\"] = LinearNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_choice = \"linear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter = {\n",
    "    \"learnrate\" : (0.01,0.1),\n",
    "    \"epoch\" : (2,4)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, optimization criterion, optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = {}\n",
    "for lr in hyperparameter[\"learnrate\"]:\n",
    "    optimizer[lr] = torch.optim.Adam(net[net_choice].parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingsloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output every running_loss_count\n",
    "running_loss_count = 50\n",
    "metrics = {\"running_loss\" : [],\n",
    "          \"validation_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate the net with 2 epochs and a learning rate of 0.01\n",
      "(Epoch 1, batch 50): 22.06590925335884\n"
     ]
    }
   ],
   "source": [
    "# save starting net\n",
    "torch.save(net[net_choice].state_dict(), getNetPath(net_choice, 0,0, original=True))\n",
    "\n",
    "for num_epoch in hyperparameter[\"epoch\"]:\n",
    "    for learnrate in hyperparameter[\"learnrate\"]:\n",
    "        #load the original net to start from the same parameters\n",
    "        net[net_choice].load_state_dict(torch.load(getNetPath(net_choice, 0,0, original=True)))\n",
    "        net[net_choice].train()\n",
    "        print(f\"Calculate the net with {num_epoch} epochs and a learning rate of {learnrate}\")\n",
    "        for epoch in range(num_epoch):\n",
    "            loss_running = 0.0\n",
    "            for i_batch, sample_batched in enumerate(dataloader[\"train\"]):\n",
    "                inputs, labels, _ = sample_batched\n",
    "                \n",
    "                #reset calculated gradients\n",
    "                optimizer[learnrate].zero_grad()\n",
    "                \n",
    "                #calculate the output and loss\n",
    "                outputs = net[net_choice](inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                #backpropagate loss and optimize parameter\n",
    "                loss.backward()\n",
    "                optimizer[learnrate].step()\n",
    "                \n",
    "                #track loss\n",
    "                loss_running += loss.item()\n",
    "                if i_batch % running_loss_count == running_loss_count -1:\n",
    "                    print(f\"(Epoch {epoch + 1}, batch {i_batch + 1}): {loss_running/running_loss_count}\")\n",
    "                    metrics[\"running_loss\"].append(loss_running)\n",
    "                    loss_running = 0.0\n",
    "        #save the trained net\n",
    "        torch.save(net[net_choice].state_dict(), getNetPath(net_choice, epoch,learnrate))\n",
    "        \n",
    "        #do the validation\n",
    "        net[net_choice].eval()\n",
    "        with torch.no_grad():\n",
    "            loss_running = 0.0\n",
    "            for i_batch, sample_batched in enumerate(dataloader[\"validate\"]):\n",
    "                inputs, labels, _ = sample_batched\n",
    "                \n",
    "                #calculate the output and loss\n",
    "                outputs = net[net_choice](inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                \n",
    "                #track loss\n",
    "                loss_running += loss.item()\n",
    "                if i_batch % running_loss_count == running_loss_count -1:\n",
    "                    print(f\"(Batch {i_batch + 1}): {loss_running/running_loss_count}\")\n",
    "                    metrics[\"validation_loss\"].append(loss_running)\n",
    "                    loss_running = 0.0\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
