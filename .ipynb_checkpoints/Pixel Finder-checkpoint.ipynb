{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "#from random import randrange\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()\n",
    "def get_image(i_dim, n_dim, n_pos , n_strength, pos_scale):\n",
    "    image = torch.zeros((3, i_dim,i_dim))\n",
    "    if isinstance(n_pos, torch.Tensor):\n",
    "        i_n_pos = (n_pos * pos_scale).type(torch.int)\n",
    "    else:\n",
    "        i_n_pos = (n_pos * pos_scale).astype(int)\n",
    "    #print(i_n_pos[0])\n",
    "    for c in range(3): #to get it equal over all chanels \n",
    "        image[c, i_n_pos[0].item():(i_n_pos[0].item() + n_dim),i_n_pos[1].item():(i_n_pos[1].item()+n_dim)] = n_strength\n",
    "    return image\n",
    "def net_show(n_pos, i_dim=64, n_dim=3, n_strength=torch.ones((3,3)), pos_scale=62):\n",
    "    with torch.no_grad():\n",
    "        npn_pos = n_pos.numpy()\n",
    "        npn_pos = np.transpose(npn_pos, (1,2,0))\n",
    "        for batch in npn_pos:\n",
    "            red_n_pos = np.array([batch[0][0], batch[1][0]])\n",
    "            print(red_n_pos)\n",
    "            image = get_image(i_dim, n_dim, red_n_pos, n_strength, pos_scale)\n",
    "            image_show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dim = 64\n",
    "n_dim = 3\n",
    "PATH_DATA = './pixel_finder_data.pth'\n",
    "sample_size = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePositionDataset(Dataset):\n",
    "    def __init__(self, i_dim, n_dim, sample_size, path=\"\"):\n",
    "        self.i_dim = i_dim\n",
    "        self.n_dim = n_dim\n",
    "        self.pos_scale = i_dim-n_dim+1\n",
    "        if path != \"\": #load the data\n",
    "            self.load(path, i_dim, n_dim)\n",
    "            assert sample_size == self.sample_size\n",
    "        else: #create new data\n",
    "            self.sample_size = sample_size\n",
    "            self.data = []\n",
    "            for i in range(sample_size):\n",
    "                n_pos = torch.flatten(torch.rand((1,2)))\n",
    "                n_strength = torch.rand((3,3))#np.random.rand(n_dim, n_dim) #one chanel\n",
    "                self.data.append((n_pos, n_strength))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sample_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = (get_image(i_dim, n_dim, self.data[idx][0], self.data[idx][1], self.pos_scale), self.data[idx][0], self.pos_scale)\n",
    "        return sample\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.data, path)\n",
    "        \n",
    "    def load(self, path, i_dim, n_dim):\n",
    "        self.data = torch.load(path)\n",
    "        self.sample_size = len(self.data)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset\n",
    "image_dataset = ImagePositionDataset(i_dim, n_dim, sample_size)\n",
    "image_dataset.save(PATH_DATA)\n",
    "dataloader = DataLoader(image_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "image_dataset = ImagePositionDataset(i_dim, n_dim, sample_size, path=PATH_DATA)\n",
    "dataloader = DataLoader(image_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(dataloader)\n",
    "images, labels, scales = dataiter.next()\n",
    "image_show(torchvision.utils.make_grid(images))\n",
    "#net(images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image,_,_ = image_dataset[0]\n",
    "image_show(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(image.unsqueeze(0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "num_epoch = 2\n",
    "PATH = './pixel_finder.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "\n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False)\n",
    "\n",
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels\n",
    "    \n",
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n",
    "    \n",
    "    \n",
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n",
    "\n",
    "\n",
    "\n",
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )\n",
    "    \n",
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )\n",
    "        \n",
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv = ResNetLayer(3, 16, block=ResNetBasicBlock, n=2)\n",
    "        self.fc1 = nn.Linear(32*32*16 , 1024)\n",
    "        self.fc2 = nn.Linear(1024,2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.drop_out = nn.Dropout(p=0.2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sig(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\". \n\tUnexpected key(s) in state_dict: \"layer1.4.weight\", \"layer1.4.bias\", \"layer1.2.weight\", \"layer1.2.bias\", \"layer2.4.weight\", \"layer2.4.bias\", \"layer2.2.weight\", \"layer2.2.bias\", \"layer3.4.weight\", \"layer3.4.bias\", \"layer3.2.weight\", \"layer3.2.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([2, 3456]).\n\tsize mismatch for layer1.0.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([24, 3, 3, 3]).\n\tsize mismatch for layer1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n\tsize mismatch for layer2.0.weight: copying a param with shape torch.Size([64, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([48, 24, 3, 3]).\n\tsize mismatch for layer2.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([48]).\n\tsize mismatch for layer3.0.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).\n\tsize mismatch for layer3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-2ec01619ccd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\", \"conv3.weight\", \"conv3.bias\". \n\tUnexpected key(s) in state_dict: \"layer1.4.weight\", \"layer1.4.bias\", \"layer1.2.weight\", \"layer1.2.bias\", \"layer2.4.weight\", \"layer2.4.bias\", \"layer2.2.weight\", \"layer2.2.bias\", \"layer3.4.weight\", \"layer3.4.bias\", \"layer3.2.weight\", \"layer3.2.bias\". \n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([2, 512]) from checkpoint, the shape in current model is torch.Size([2, 3456]).\n\tsize mismatch for layer1.0.weight: copying a param with shape torch.Size([16, 3, 5, 5]) from checkpoint, the shape in current model is torch.Size([24, 3, 3, 3]).\n\tsize mismatch for layer1.0.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([24]).\n\tsize mismatch for layer2.0.weight: copying a param with shape torch.Size([64, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([48, 24, 3, 3]).\n\tsize mismatch for layer2.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([48]).\n\tsize mismatch for layer3.0.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([96, 48, 3, 3]).\n\tsize mismatch for layer3.0.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([96])."
     ]
    }
   ],
   "source": [
    "#Load net\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.701\n",
      "[1,   400] loss: 0.703\n",
      "[1,   600] loss: 0.721\n",
      "[1,   800] loss: 0.715\n",
      "[1,  1000] loss: 0.708\n",
      "[1,  1200] loss: 0.730\n",
      "[1,  1400] loss: 0.718\n",
      "[1,  1600] loss: 0.711\n",
      "[1,  1800] loss: 0.712\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-b0dec1308233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m199\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# print every 2000 mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'betas'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programme\\Anaconda\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    running_loss = 0.0\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        inputs,labels,_ = sample_batched\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i_batch % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i_batch + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkout Adam and AdamW as optimizer\n",
    "#find out why loss is so high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
